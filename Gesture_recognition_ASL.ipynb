{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kTWiFF2cNMpV",
    "outputId": "60ea5c05-61c3-43bd-b7e6-1473cc60e08d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.1.2\n",
      "    Uninstalling pip-23.1.2:\n",
      "      Successfully uninstalled pip-23.1.2\n",
      "Successfully installed pip-23.3\n",
      "Collecting mediapipe-model-maker\n",
      "  Downloading mediapipe_model_maker-0.2.1.3-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.4.0)\n",
      "Collecting mediapipe>=0.10.0 (from mediapipe-model-maker)\n",
      "  Downloading mediapipe-0.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.23.5)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.8.0.76)\n",
      "Requirement already satisfied: tensorflow>=2.10 in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (2.13.0)\n",
      "Collecting tensorflow-addons (from mediapipe-model-maker)\n",
      "  Downloading tensorflow_addons-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.9.3)\n",
      "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (0.15.0)\n",
      "Collecting tf-models-official>=2.13.1 (from mediapipe-model-maker)\n",
      "  Downloading tf_models_official-2.14.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (23.5.26)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.7.1)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.8.0.76)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.20.3)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe>=0.10.0->mediapipe-model-maker)\n",
      "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.59.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (3.9.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (23.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.10->mediapipe-model-maker) (0.34.0)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (3.0.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (9.4.0)\n",
      "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.84.0)\n",
      "Collecting immutabledict (from tf-models-official>=2.13.1->mediapipe-model-maker)\n",
      "  Downloading immutabledict-3.0.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.5.16)\n",
      "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (4.1.3)\n",
      "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (4.8.1.78)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.5.3)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (9.0.0)\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (6.0.1)\n",
      "Collecting sacrebleu (from tf-models-official>=2.13.1->mediapipe-model-maker)\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.11.3)\n",
      "Collecting sentencepiece (from tf-models-official>=2.13.1->mediapipe-model-maker)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting seqeval (from tf-models-official>=2.13.1->mediapipe-model-maker)\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official>=2.13.1->mediapipe-model-maker)\n",
      "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
      "Collecting tensorflow-text~=2.14.0 (from tf-models-official>=2.13.1->mediapipe-model-maker)\n",
      "  Downloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting tensorflow>=2.10 (from mediapipe-model-maker)\n",
      "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.13.1->mediapipe-model-maker) (1.1.0)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow>=2.10->mediapipe-model-maker)\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow>=2.10->mediapipe-model-maker)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.15,>=2.14 (from tensorflow>=2.10->mediapipe-model-maker)\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow>=2.10->mediapipe-model-maker)\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow>=2.10->mediapipe-model-maker)\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->mediapipe-model-maker)\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.4.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.1.8)\n",
      "Requirement already satisfied: etils>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (1.5.1)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (2.31.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (1.14.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->mediapipe-model-maker) (4.66.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.10->mediapipe-model-maker) (0.41.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (2023.6.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (6.1.0)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->mediapipe-model-maker) (3.17.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (2.17.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (0.1.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (2.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (4.1.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (8.0.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (2.0.7)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (6.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.3.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.4)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (1.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.10->mediapipe-model-maker) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.10->mediapipe-model-maker) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.10->mediapipe-model-maker) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow>=2.10->mediapipe-model-maker) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (3.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (0.3.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.13.1->mediapipe-model-maker) (4.9)\n",
      "Collecting portalocker (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (2023.6.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (0.9.0)\n",
      "Collecting colorama (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->tf-models-official>=2.13.1->mediapipe-model-maker) (4.9.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (1.2.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->mediapipe-model-maker) (1.61.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (2.21)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official>=2.13.1->mediapipe-model-maker) (5.3.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow>=2.10->mediapipe-model-maker) (1.3.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.13.1->mediapipe-model-maker) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow>=2.10->mediapipe-model-maker) (2.1.3)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.13.1->mediapipe-model-maker) (1.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow>=2.10->mediapipe-model-maker) (3.2.2)\n",
      "Downloading mediapipe_model_maker-0.2.1.3-py3-none-any.whl (127 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mediapipe-0.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tf_models_official-2.14.2-py2.py3-none-any.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_addons-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.3/612.3 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading immutabledict-3.0.0-py3-none-any.whl (4.0 kB)\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=9450c061b6e03cf9ea2cfec2686d046393d6275ef5d115ffce34925ba578ae7b\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: sentencepiece, wrapt, typeguard, tensorflow-model-optimization, tensorflow-estimator, portalocker, ml-dtypes, keras, immutabledict, colorama, tensorflow-addons, sounddevice, sacrebleu, seqeval, mediapipe, tensorboard, tensorflow, tensorflow-text, tf-models-official, mediapipe-model-maker\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.15.0\n",
      "    Uninstalling wrapt-1.15.0:\n",
      "      Successfully uninstalled wrapt-1.15.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.13.0\n",
      "    Uninstalling tensorflow-estimator-2.13.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.3.1\n",
      "    Uninstalling ml-dtypes-0.3.1:\n",
      "      Successfully uninstalled ml-dtypes-0.3.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.13.1\n",
      "    Uninstalling keras-2.13.1:\n",
      "      Successfully uninstalled keras-2.13.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.13.0\n",
      "    Uninstalling tensorboard-2.13.0:\n",
      "      Successfully uninstalled tensorboard-2.13.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.13.0\n",
      "    Uninstalling tensorflow-2.13.0:\n",
      "      Successfully uninstalled tensorflow-2.13.0\n",
      "Successfully installed colorama-0.4.6 immutabledict-3.0.0 keras-2.14.0 mediapipe-0.10.7 mediapipe-model-maker-0.2.1.3 ml-dtypes-0.2.0 portalocker-2.8.2 sacrebleu-2.3.1 sentencepiece-0.1.99 seqeval-1.2.2 sounddevice-0.4.6 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-addons-0.22.0 tensorflow-estimator-2.14.0 tensorflow-model-optimization-0.7.5 tensorflow-text-2.14.0 tf-models-official-2.14.2 typeguard-2.13.3 wrapt-1.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install mediapipe-model-maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzfKho1FNfQi",
    "outputId": "28733c2e-be02-4414-af29-ca671e60f6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-JeKPdCNye1",
    "outputId": "be1df6de-c57e-4a4f-abe1-1e2c6d8d7bd3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "from mediapipe_model_maker import gesture_recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czadRaJEWruI",
    "outputId": "41f009f5-2c90-4b47-9a15-24ec3566cba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZL9IEE0tQb_3",
    "outputId": "ae0b8021-d5fb-4112-dcd7-79402c6cd632"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset\n",
      "['x', 'z', 't', 'q', 'u', 'y', 's', 'w', 'v', 'r', 'n', 'p', 'o', 'k', 'j', 'h', 'm', 'i', 'l', 'g', '8', '7', 'c', 'a', 'b', '9', 'f', 'd', 'e', '0', '2', '6', '1', '5', '3', '4', 'None']\n"
     ]
    }
   ],
   "source": [
    "#creating labels\n",
    "dataset_path ='/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset'\n",
    "print(dataset_path)\n",
    "labels = []\n",
    "for i in os.listdir(dataset_path):\n",
    "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
    "    labels.append(i)\n",
    "print(labels)\n",
    "labels.append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s47Eq9DpRZQn",
    "outputId": "e9cbdec0-d057-41db-a206-224d1ef40b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/x\n",
      "['hand1_x_bot_seg_3_cropped.jpeg', 'hand2_x_dif_seg_4_cropped.jpeg', 'hand2_x_dif_seg_3_cropped.jpeg', 'hand1_x_bot_seg_5_cropped.jpeg', 'hand2_x_top_seg_2_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/z\n",
      "['hand1_z_bot_seg_1_cropped.jpeg', 'hand1_z_bot_seg_4_cropped.jpeg', 'hand1_z_bot_seg_3_cropped.jpeg', 'hand1_z_bot_seg_2_cropped.jpeg', 'hand1_z_left_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/t\n",
      "['hand1_t_bot_seg_3_cropped.jpeg', 'hand1_t_left_seg_5_cropped.jpeg', 'hand1_t_right_seg_3_cropped.jpeg', 'hand1_t_dif_seg_2_cropped.jpeg', 'hand1_t_dif_seg_4_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/q\n",
      "['hand1_q_bot_seg_5_cropped.jpeg', 'hand1_q_bot_seg_4_cropped.jpeg', 'hand1_q_dif_seg_5_cropped.jpeg', 'hand2_q_left_seg_4_cropped.jpeg', 'hand1_q_left_seg_2_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/u\n",
      "['hand1_u_dif_seg_1_cropped.jpeg', 'hand1_u_bot_seg_4_cropped.jpeg', 'hand1_u_bot_seg_5_cropped.jpeg', 'hand1_u_dif_seg_2_cropped.jpeg', 'hand1_u_bot_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/y\n",
      "['hand1_y_dif_seg_4_cropped.jpeg', 'hand1_y_bot_seg_1_cropped.jpeg', 'hand1_y_dif_seg_3_cropped.jpeg', 'hand1_y_dif_seg_1_cropped.jpeg', 'hand1_y_top_seg_2_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/s\n",
      "['hand1_s_right_seg_1_cropped.jpeg', 'hand1_s_right_seg_4_cropped.jpeg', 'hand2_s_dif_seg_2_cropped.jpeg', 'hand2_s_left_seg_2_cropped.jpeg', 'hand1_s_bot_seg_5_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/w\n",
      "['hand1_w_left_seg_2_cropped.jpeg', 'hand1_w_right_seg_3_cropped.jpeg', 'hand1_w_bot_seg_3_cropped.jpeg', 'hand1_w_dif_seg_4_cropped.jpeg', 'hand1_w_dif_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/v\n",
      "['hand2_v_bot_seg_4_cropped.jpeg', 'hand1_v_left_seg_5_cropped.jpeg', 'hand1_v_bot_seg_5_cropped.jpeg', 'hand1_v_right_seg_2_cropped.jpeg', 'hand2_v_left_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/r\n",
      "['hand1_r_dif_seg_3_cropped.jpeg', 'hand1_r_left_seg_3_cropped.jpeg', 'hand1_r_bot_seg_5_cropped.jpeg', 'hand1_r_bot_seg_2_cropped.jpeg', 'hand1_r_dif_seg_4_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/n\n",
      "['hand1_n_bot_seg_2_cropped.jpeg', 'hand1_n_bot_seg_4_cropped.jpeg', 'hand1_n_bot_seg_5_cropped.jpeg', 'hand1_n_bot_seg_3_cropped.jpeg', 'hand1_n_bot_seg_1_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/p\n",
      "['hand1_p_dif_seg_1_cropped.jpeg', 'hand1_p_dif_seg_5_cropped.jpeg', 'hand1_p_bot_seg_2_cropped.jpeg', 'hand1_p_bot_seg_5_cropped.jpeg', 'hand1_p_bot_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/o\n",
      "['hand1_o_left_seg_5_cropped.jpeg', 'hand1_o_top_seg_4_cropped.jpeg', 'hand1_o_right_seg_3_cropped.jpeg', 'hand1_o_left_seg_2_cropped.jpeg', 'hand1_o_dif_seg_2_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/k\n",
      "['hand1_k_dif_seg_4_cropped.jpeg', 'hand1_k_left_seg_4_cropped.jpeg', 'hand1_k_right_seg_5_cropped.jpeg', 'hand1_k_top_seg_1_cropped.jpeg', 'hand1_k_dif_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/j\n",
      "['hand2_j_bot_seg_3_cropped.jpeg', 'hand1_j_top_seg_2_cropped.jpeg', 'hand2_j_left_seg_1_cropped.jpeg', 'hand1_j_bot_seg_4_cropped.jpeg', 'hand2_j_right_seg_1_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/h\n",
      "['hand1_h_bot_seg_3_cropped.jpeg', 'hand1_h_right_seg_4_cropped.jpeg', 'hand1_h_dif_seg_2_cropped.jpeg', 'hand1_h_top_seg_3_cropped.jpeg', 'hand1_h_bot_seg_4_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/m\n",
      "['hand1_m_bot_seg_4_cropped.jpeg', 'hand1_m_right_seg_4_cropped.jpeg', 'hand1_m_top_seg_4_cropped.jpeg', 'hand1_m_left_seg_4_cropped.jpeg', 'hand1_m_left_seg_5_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/i\n",
      "['hand1_i_dif_seg_5_cropped.jpeg', 'hand1_i_bot_seg_3_cropped.jpeg', 'hand1_i_bot_seg_2_cropped.jpeg', 'hand1_i_dif_seg_1_cropped.jpeg', 'hand1_i_left_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/l\n",
      "['hand1_l_bot_seg_3_cropped.jpeg', 'hand1_l_right_seg_5_cropped.jpeg', 'hand2_l_left_seg_1_cropped.jpeg', 'hand1_l_bot_seg_2_cropped.jpeg', 'hand1_l_top_seg_2_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/g\n",
      "['hand1_g_dif_seg_3_cropped.jpeg', 'hand1_g_dif_seg_2_cropped.jpeg', 'hand1_g_bot_seg_2_cropped.jpeg', 'hand1_g_bot_seg_5_cropped.jpeg', 'hand1_g_bot_seg_4_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/8\n",
      "['hand1_8_top_seg_3_cropped.jpeg', 'hand2_8_bot_seg_3_cropped.jpeg', 'hand1_8_left_seg_2_cropped.jpeg', 'hand1_8_bot_seg_5_cropped.jpeg', 'hand1_8_bot_seg_2_cropped.jpeg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-905e0d5e9c01>:7: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/7\n",
      "['hand1_7_bot_seg_3_cropped.jpeg', 'hand1_7_dif_seg_4_cropped.jpeg', 'hand1_7_dif_seg_3_cropped.jpeg', 'hand1_7_left_seg_1_cropped.jpeg', 'hand1_7_left_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/c\n",
      "['hand1_c_top_seg_4_cropped.jpeg', 'hand2_c_left_seg_2_cropped.jpeg', 'hand1_c_right_seg_3_cropped.jpeg', 'hand1_c_top_seg_2_cropped.jpeg', 'hand1_c_dif_seg_5_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/a\n",
      "['hand1_a_bot_seg_2_cropped.jpeg', 'hand1_a_bot_seg_3_cropped.jpeg', 'hand1_a_bot_seg_5_cropped.jpeg', 'hand1_a_bot_seg_4_cropped.jpeg', 'hand1_a_dif_seg_4_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/b\n",
      "['hand1_b_bot_seg_5_cropped.jpeg', 'hand1_b_dif_seg_2_cropped.jpeg', 'hand1_b_bot_seg_2_cropped.jpeg', 'hand1_b_bot_seg_1_cropped.jpeg', 'hand1_b_dif_seg_1_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/9\n",
      "['hand1_9_left_seg_4_cropped.jpeg', 'hand1_9_left_seg_3_cropped.jpeg', 'hand1_9_top_seg_1_cropped.jpeg', 'hand1_9_dif_seg_3_cropped.jpeg', 'hand1_9_right_seg_4_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/f\n",
      "['hand1_f_right_seg_1_cropped.jpeg', 'hand1_f_dif_seg_3_cropped.jpeg', 'hand1_f_top_seg_4_cropped.jpeg', 'hand1_f_top_seg_5_cropped.jpeg', 'hand1_f_bot_seg_4_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/d\n",
      "['hand1_d_dif_seg_3_cropped.jpeg', 'hand1_d_bot_seg_1_cropped.jpeg', 'hand1_d_left_seg_1_cropped.jpeg', 'hand1_d_right_seg_2_cropped.jpeg', 'hand1_d_bot_seg_2_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/e\n",
      "['hand2_e_dif_seg_5_cropped.jpeg', 'hand2_e_dif_seg_1_cropped.jpeg', 'hand2_e_bot_seg_3_cropped.jpeg', 'hand2_e_top_seg_5_cropped.jpeg', 'hand2_e_left_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/0\n",
      "['hand1_0_bot_seg_1_cropped.jpeg', 'hand1_0_bot_seg_3_cropped.jpeg', 'hand1_0_dif_seg_1_cropped.jpeg', 'hand1_0_bot_seg_5_cropped.jpeg', 'hand1_0_bot_seg_2_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/2\n",
      "['hand1_2_bot_seg_1_cropped.jpeg', 'hand1_2_dif_seg_3_cropped.jpeg', 'hand1_2_dif_seg_4_cropped.jpeg', 'hand1_2_bot_seg_2_cropped.jpeg', 'hand1_2_dif_seg_1_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/6\n",
      "['hand1_6_dif_seg_4_cropped.jpeg', 'hand1_6_dif_seg_1_cropped.jpeg', 'hand2_6_bot_seg_3_cropped.jpeg', 'hand1_6_right_seg_4_cropped.jpeg', 'hand1_6_top_seg_1_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/1\n",
      "['hand1_1_top_seg_4_cropped.jpeg', 'hand1_1_right_seg_1_cropped.jpeg', 'hand1_1_top_seg_5_cropped.jpeg', 'hand1_1_right_seg_4_cropped.jpeg', 'hand1_1_left_seg_4_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/5\n",
      "['hand1_5_bot_seg_2_cropped.jpeg', 'hand1_5_bot_seg_1_cropped.jpeg', 'hand2_5_right_seg_3_cropped.jpeg', 'hand2_5_top_seg_2_cropped.jpeg', 'hand2_5_top_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/3\n",
      "['hand1_3_right_seg_2_cropped.jpeg', 'hand1_3_left_seg_3_cropped.jpeg', 'hand2_3_left_seg_4_cropped.jpeg', 'hand1_3_left_seg_5_cropped.jpeg', 'hand1_3_right_seg_3_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/4\n",
      "['hand1_4_top_seg_1_cropped.jpeg', 'hand1_4_dif_seg_3_cropped.jpeg', 'hand1_4_left_seg_3_cropped.jpeg', 'hand1_4_bot_seg_4_cropped.jpeg', 'hand1_4_bot_seg_1_cropped.jpeg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/None\n",
      "['1488.jpg', '255.jpg', '1712.jpg', '1885.jpg', '800.jpg']\n",
      "/content/drive/MyDrive/Hand-gesture-recognition-GAN/Hand-gesture-recognition-GAN/asl_dataset/None\n",
      "['1488.jpg', '255.jpg', '1712.jpg', '1885.jpg', '800.jpg']\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 5\n",
    "\n",
    "for label in labels:\n",
    "  label_dir = os.path.join(dataset_path, label)\n",
    "  print(label_dir)\n",
    "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
    "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
    "  print(example_filenames)\n",
    "  for i in range(NUM_EXAMPLES):\n",
    "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
    "    axs[i].get_xaxis().set_visible(False)\n",
    "    axs[i].get_yaxis().set_visible(False)\n",
    "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzQuF2ZjRgwC",
    "outputId": "c26ea801-63f9-4123-e08c-ed8366582f93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://storage.googleapis.com/mediapipe-assets/palm_detection_full.tflite to /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
      "Downloading https://storage.googleapis.com/mediapipe-assets/hand_landmark_full.tflite to /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
      "Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tar.gz to /tmp/model_maker/gesture_recognizer/gesture_embedder\n"
     ]
    }
   ],
   "source": [
    "data = gesture_recognizer.Dataset.from_folder(\n",
    "    dirname=dataset_path,\n",
    "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
    ")\n",
    "train_data, rest_data = data.split(0.8)\n",
    "validation_data, test_data = rest_data.split(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQCXcn9gS2S8",
    "outputId": "97ea0bcc-0770-4947-c7c9-16996a8174fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hand_embedding (InputLayer  [(None, 128)]             0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " custom_gesture_recognizer_  (None, 64)                8256      \n",
      " 0 (Dense)                                                       \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " custom_gesture_recognizer_  (None, 128)               8320      \n",
      " 1 (Dense)                                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_5 (ReLU)              (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " custom_gesture_recognizer_  (None, 37)                4773      \n",
      " out (Dense)                                                     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22629 (88.39 KB)\n",
      "Trainable params: 21989 (85.89 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Resuming from exported_model/epoch_models/model-0030\n",
      "Epoch 1/40\n",
      "152/152 [==============================] - 4s 15ms/step - loss: 0.2461 - categorical_accuracy: 0.7845 - val_loss: 0.3647 - val_categorical_accuracy: 0.7974 - lr: 0.0050\n",
      "Epoch 2/40\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.3001 - categorical_accuracy: 0.7533 - val_loss: 0.2298 - val_categorical_accuracy: 0.8431 - lr: 0.0047\n",
      "Epoch 3/40\n",
      "152/152 [==============================] - 2s 14ms/step - loss: 0.2538 - categorical_accuracy: 0.7804 - val_loss: 0.1912 - val_categorical_accuracy: 0.8366 - lr: 0.0045\n",
      "Epoch 4/40\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.2354 - categorical_accuracy: 0.7812 - val_loss: 0.2485 - val_categorical_accuracy: 0.8366 - lr: 0.0043\n",
      "Epoch 5/40\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.2002 - categorical_accuracy: 0.7919 - val_loss: 0.1765 - val_categorical_accuracy: 0.8562 - lr: 0.0041\n",
      "Epoch 6/40\n",
      "152/152 [==============================] - 3s 16ms/step - loss: 0.1873 - categorical_accuracy: 0.8059 - val_loss: 0.2224 - val_categorical_accuracy: 0.8431 - lr: 0.0039\n",
      "Epoch 7/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.1668 - categorical_accuracy: 0.8232 - val_loss: 0.2459 - val_categorical_accuracy: 0.8366 - lr: 0.0037\n",
      "Epoch 8/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.1793 - categorical_accuracy: 0.8133 - val_loss: 0.2128 - val_categorical_accuracy: 0.8170 - lr: 0.0035\n",
      "Epoch 9/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.1527 - categorical_accuracy: 0.8331 - val_loss: 0.2243 - val_categorical_accuracy: 0.8039 - lr: 0.0033\n",
      "Epoch 10/40\n",
      "152/152 [==============================] - 2s 15ms/step - loss: 0.1593 - categorical_accuracy: 0.8281 - val_loss: 0.1829 - val_categorical_accuracy: 0.8366 - lr: 0.0032\n",
      "Epoch 11/40\n",
      "152/152 [==============================] - 4s 24ms/step - loss: 0.1772 - categorical_accuracy: 0.8289 - val_loss: 0.1945 - val_categorical_accuracy: 0.8693 - lr: 0.0030\n",
      "Epoch 12/40\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.1762 - categorical_accuracy: 0.8150 - val_loss: 0.1721 - val_categorical_accuracy: 0.8824 - lr: 0.0028\n",
      "Epoch 13/40\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.1745 - categorical_accuracy: 0.8174 - val_loss: 0.1665 - val_categorical_accuracy: 0.8758 - lr: 0.0027\n",
      "Epoch 14/40\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.1367 - categorical_accuracy: 0.8421 - val_loss: 0.1555 - val_categorical_accuracy: 0.8562 - lr: 0.0026\n",
      "Epoch 15/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.1441 - categorical_accuracy: 0.8339 - val_loss: 0.2058 - val_categorical_accuracy: 0.7908 - lr: 0.0024\n",
      "Epoch 16/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.1345 - categorical_accuracy: 0.8544 - val_loss: 0.1538 - val_categorical_accuracy: 0.8627 - lr: 0.0023\n",
      "Epoch 17/40\n",
      "152/152 [==============================] - 4s 22ms/step - loss: 0.1356 - categorical_accuracy: 0.8322 - val_loss: 0.1646 - val_categorical_accuracy: 0.8562 - lr: 0.0022\n",
      "Epoch 18/40\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.1307 - categorical_accuracy: 0.8569 - val_loss: 0.1678 - val_categorical_accuracy: 0.8562 - lr: 0.0021\n",
      "Epoch 19/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.1287 - categorical_accuracy: 0.8479 - val_loss: 0.1671 - val_categorical_accuracy: 0.8824 - lr: 0.0020\n",
      "Epoch 20/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.1431 - categorical_accuracy: 0.8388 - val_loss: 0.1694 - val_categorical_accuracy: 0.8758 - lr: 0.0019\n",
      "Epoch 21/40\n",
      "152/152 [==============================] - 3s 19ms/step - loss: 0.1213 - categorical_accuracy: 0.8553 - val_loss: 0.1589 - val_categorical_accuracy: 0.8889 - lr: 0.0018\n",
      "Epoch 22/40\n",
      "152/152 [==============================] - 4s 25ms/step - loss: 0.1247 - categorical_accuracy: 0.8495 - val_loss: 0.1651 - val_categorical_accuracy: 0.8431 - lr: 0.0017\n",
      "Epoch 23/40\n",
      "152/152 [==============================] - 4s 24ms/step - loss: 0.1160 - categorical_accuracy: 0.8429 - val_loss: 0.1715 - val_categorical_accuracy: 0.8693 - lr: 0.0016\n",
      "Epoch 24/40\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.1128 - categorical_accuracy: 0.8495 - val_loss: 0.1715 - val_categorical_accuracy: 0.8627 - lr: 0.0015\n",
      "Epoch 25/40\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.1191 - categorical_accuracy: 0.8512 - val_loss: 0.1819 - val_categorical_accuracy: 0.8431 - lr: 0.0015\n",
      "Epoch 26/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.1213 - categorical_accuracy: 0.8479 - val_loss: 0.1787 - val_categorical_accuracy: 0.8497 - lr: 0.0014\n",
      "Epoch 27/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.0964 - categorical_accuracy: 0.8610 - val_loss: 0.1752 - val_categorical_accuracy: 0.8627 - lr: 0.0013\n",
      "Epoch 28/40\n",
      "152/152 [==============================] - 3s 19ms/step - loss: 0.1060 - categorical_accuracy: 0.8553 - val_loss: 0.1671 - val_categorical_accuracy: 0.8497 - lr: 0.0013\n",
      "Epoch 29/40\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.1132 - categorical_accuracy: 0.8602 - val_loss: 0.1619 - val_categorical_accuracy: 0.8366 - lr: 0.0012\n",
      "Epoch 30/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.0969 - categorical_accuracy: 0.8520 - val_loss: 0.1533 - val_categorical_accuracy: 0.8824 - lr: 0.0011\n",
      "Epoch 31/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.0914 - categorical_accuracy: 0.8701 - val_loss: 0.1471 - val_categorical_accuracy: 0.8431 - lr: 0.0011\n",
      "Epoch 32/40\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.0979 - categorical_accuracy: 0.8627 - val_loss: 0.1582 - val_categorical_accuracy: 0.8562 - lr: 0.0010\n",
      "Epoch 33/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.1070 - categorical_accuracy: 0.8487 - val_loss: 0.1536 - val_categorical_accuracy: 0.8627 - lr: 9.6856e-04\n",
      "Epoch 34/40\n",
      "152/152 [==============================] - 3s 22ms/step - loss: 0.1034 - categorical_accuracy: 0.8577 - val_loss: 0.1512 - val_categorical_accuracy: 0.8693 - lr: 9.2013e-04\n",
      "Epoch 35/40\n",
      "152/152 [==============================] - 4s 23ms/step - loss: 0.0946 - categorical_accuracy: 0.8602 - val_loss: 0.1550 - val_categorical_accuracy: 0.8693 - lr: 8.7412e-04\n",
      "Epoch 36/40\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.0860 - categorical_accuracy: 0.8684 - val_loss: 0.1387 - val_categorical_accuracy: 0.8562 - lr: 8.3042e-04\n",
      "Epoch 37/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.0938 - categorical_accuracy: 0.8791 - val_loss: 0.1428 - val_categorical_accuracy: 0.8693 - lr: 7.8890e-04\n",
      "Epoch 38/40\n",
      "152/152 [==============================] - 2s 12ms/step - loss: 0.0961 - categorical_accuracy: 0.8766 - val_loss: 0.1413 - val_categorical_accuracy: 0.8562 - lr: 7.4945e-04\n",
      "Epoch 39/40\n",
      "152/152 [==============================] - 2s 13ms/step - loss: 0.0834 - categorical_accuracy: 0.8725 - val_loss: 0.1418 - val_categorical_accuracy: 0.8889 - lr: 7.1198e-04\n",
      "Epoch 40/40\n",
      "152/152 [==============================] - 3s 19ms/step - loss: 0.0927 - categorical_accuracy: 0.8766 - val_loss: 0.1435 - val_categorical_accuracy: 0.8954 - lr: 6.7638e-04\n"
     ]
    }
   ],
   "source": [
    "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\",\n",
    "learning_rate=0.005,\n",
    "    batch_size=8,\n",
    "    epochs=40,\n",
    "    steps_per_epoch=None,\n",
    "    shuffle=True,\n",
    "    lr_decay=0.95,\n",
    "    gamma=3.0)\n",
    "model_options = gesture_recognizer.ModelOptions(\n",
    "    dropout_rate=0.1,\n",
    "    layer_widths=[64, 128]\n",
    ")\n",
    "options = gesture_recognizer.GestureRecognizerOptions(model_options = model_options, hparams=hparams)\n",
    "model = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    options=options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aYQJpSn84UWC",
    "outputId": "cd1eedc3-b2a8-4d64-e1c8-b7812397edf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 2s 3ms/step - loss: 0.1435 - categorical_accuracy: 0.8954\n",
      "Validation loss:0.14351199567317963, Validation accuracy:0.8954248428344727\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(validation_data, batch_size=1)\n",
    "print(f\"Validation loss:{loss}, Validation accuracy:{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJTC7Rjc4ocx",
    "outputId": "47b8e689-e263-4693-8aed-06131b83d075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 3s 4ms/step - loss: 0.0565 - categorical_accuracy: 0.9608\n",
      "Test loss:0.056541699916124344, Test accuracy:0.9607843160629272\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_data, batch_size=1)\n",
    "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_1DV8uQPmt0",
    "outputId": "3c250022-eafb-47d9-f844-2fd9a3028562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tflite to /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
      "Downloading https://storage.googleapis.com/mediapipe-assets/canned_gesture_classifier.tflite to /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n",
      "best_model_weights.data-00000-of-00001\tcheckpoint    gesture_recognizer.task  metadata.json\n",
      "best_model_weights.index\t\tepoch_models  logs\n"
     ]
    }
   ],
   "source": [
    "model.export_model()\n",
    "!ls exported_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "uj63X3jxPtK_",
    "outputId": "435bfb5c-09eb-4af5-d96c-35f030391953"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_7c5fa972-1621-42cd-b9fe-1a26f5c37400\", \"gesture_recognizer.task\", 8545419)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download('exported_model/gesture_recognizer.task')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
